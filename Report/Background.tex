
\section{Background}
\subsection{Functional Programming}
Functional programming is a programming paradigm distinct from imperative
programming. It aims to view programs more as mathematical functions or series
thereof and less as sequences of high level machine instructions. The concepts
of global state and stateful computations are avoided. Where necessary, states
are represented as values passed between stateless functions. Such stateless
programs give rise to the notion of \emph{purity}. This is the concept in many functional
languages that states that programs will always evaluate to the same result unless
stateful operations are intentionally added. Pure functions are often referrred
to as \emph{referentially transparent}. Referential transparency of an expression in a
program allows us to say that replacing the expression with its value will cause
no change in the execution of the program as a whole. Lazy evaluation is an
evaluation strategy frequently found in functional programming. Distinct from
strict evaluation, Lazy evaluation operates on a \emph{call-by-need} basis, only
evaluating aspects of an expression as they are required. A secondary characteristic
in lazy evaluation is that of \emph{updating}. Updating puts referential
transparity to practice by rewriting expressions with their evaluated results after
their first evaluation. Many functional languages can be seen as abstractions built
on top of (polymorphically typed) lambda calculus~\cite[pp.37]{SPJ}, examples including 
Haskell, Miranda, ML and Lisp. This is particularly noticeable in Lisp where the 
influence of lambda calculus is still visible in its syntax. This is of course 
an extremely high-level overview of functional programming. Where possible I will attempt to explain 
any of the above mentioned concepts which appear frequently throughout this report. 

\subsection{Functional Compilation}
I will be referring frequently to some of the more common ideas and practices
involved in compiling a functional language throughout this report. Unsurprisingly
the field of functional compiler design is not easily summarized in a paragraph,
let alone a paper; the implementation explained here only scratches the surface.
However it will be easier to understand some of the points I make if we start
with a quick overview.

Most compiler implementations for simple functional languages start with an 
input language which consists of a list of function definitions and some entry
or \emph{main} function, which serves as the entry point for the runtime or evaluator.
Some of the common components we would expect to see in such a language are:

\begin{enumerate}
	\item Variable names corresponding to a defined value.
	\item Primitive integers.
	\item Data constructors, consisting of a number of elements and a 
		  tag to uniquely identify a constructor in its own context.
	\item Binary function applicators which we use to apply expressions in order
		  to construct larger expressions.
	\item Let expressions, allowing us to define a list of arguments local
		  to a specific expression.
	\item Case expressions, containing first a condition expression which
		  will evaluate to a tag identifying a data constructor in the context
		  of the case expression, and secondly, a list of constructor tags and 
		  the code to be executed when a particular tag is found.
	\item Lambda abstractions, or anonymous expressions. 
\end{enumerate}

\noindent We can combine the above components to form expressions in our language. The
term \emph{supercombinator} is used to refer to a named expression with a 
list of \emph{binders} or arguments~\cite[pp.12]{Tutorial}. We could compare these to function 
definitions in more conventional terminology. With these supercombinator
defintions we are capable of building programs. A programmatic representation
of such a language would look like this:

\begin{verbatim}
Program ::= [Var, [a], Expr a] 	-- Program ::= [Supercombinator]

Expr a
  ::= EVar Name
    | ENum Int
    | EConstr Int Int 			-- data constructors
    | EAp (Expr a) (Expr a) 	-- expression applicators
    | ELet 
        IsRec
        [(a, Expr a)]
        (Expr a)
    | ECase
        (Expr a)
        [Alter a]
    | ELam [a] (Expr a) 		-- lambda abstractions
\end{verbatim}

\noindent By way of example, a sample program taking two parameters and returning
their sum, written in such a language, would resemble

\begin{verbatim}
 ["sum"], ["x", "y"], EAp (EAp (EVar "+") (EVar "x")) (EVar "y")
\end{verbatim}

\noindent Note in particular that we address a supercombinator named \verb!"+"!
representing our integer addition primitive and that we use two EAp 
binary expression applications in order to apply it to two parameters x and y.

Our compiler will take a program in such a simplified representation
and convert it into an initial state representing our input program and the
means to evaluate it to a final state. Depending on the compiler implementation
in particular, this may consist just of an initial state or an initial state
and a sequence of instructions denoting actions to be executed on the state. 
An interesting 
distinction between these compilation schemes and those of imperative languages
is that we can view our initial (and continuing) program states as graphs built
from a small set of nodes~\cite[pp.185]{SPJ}, namely

\begin{enumerate}
	\item Integer nodes, representing literal integer values
	\item Function nodes consisting of the arity of the function and the
		  its behaviour along with the instructions needed to evaluate it.
	\item Application functions, the only nodes capable of forming 
		  node connections from a graph point of view and used to represent
		  expression applications in terms of our simplified expr language.
	\item Data Constructor nodes, consisting of the arity of the data
		  constructor and a list of addresses to the nodes containing its
		  elements.
\end{enumerate}

We say that our program can be represented by a graph of reducible expressions
or \emph{redexes} built from the above nodes. 
Our program state will consist of a heap containing the nodes which form
our graph addressable by their address in the heap. We will
also have a stack used to contain a working set of heap addresses which 
will be required in the evaluation of immediately pending expressions. A 
\emph{globals} dictionary will associate global supercombinator identifiers with the
addresses of their representative nodes in the heap. \\

An evaluator or runtime evaluates our compiled program by iterating through
states from the initial state to the final. On each iteration, the state
(and instruction sequence if present) are examined to determine the actions
to execute in order to reach the next state. The appropriate actions are
executed, producing a next state which we will then evaluate in the same
manner. Expressions are instantiated as redexes in our heap which we reduce
to find their resultant values. We evaluate the program as a whole in this
fashion to a final result.

\subsection{Lazy Evaluation}
Lazy evaluation, also known as call-by-need evaluation, is an evaluation method
that delays evaluations until such time as their values are required~\cite[pp.33]{SPJ}. 
The result of this is that a computation we write will only be 
evaluated when it is required by some other aspect of the program, or possibly never
if its value is never required. 

This is useful both from an efficiency point of view and when trying to 
represent concepts which might not fit into stricter, more finite ideas of
programming. Let's take two examples:

\begin{center}
	\verb!take 1 ['a'..'e']!
\end{center}

\noindent Here, \verb!['a'..'z']! represents a list of characters beginning with 'a' and 
ending with 'e'. The function \verb!take! evaluates and returns the number of
elements of the second parameter specified by the first parameter, starting at the
head of the list. In this example we're \emph{taking} one element from "abcde" which
gives us the character 'a'. 

\begin{center}
	\verb!take 1 [1..]!
\end{center}

\noindent Here, \verb![1..]! is the list of integers beginning with 1 and continuing 
to infinity. In both examples, the process for evaluating  \verb!take! is the same. 
The square bracket
syntax is used to construct lists containing the items within the brackets as usual.
The \verb!..! syntax identifies a pattern between the items mentioned within the
bracket and will construct a list as defined by that pattern. However, the list is
not constructed in advance. In the context of functional programming languages such
as Haskell, we view lists as concatenation functions on elements. When we call
\verb!take! on a list, we evaluate this function as many times as
necessary to produce the number of elements required. In this manner, we do not
need to know what items lay beyond the first element in the list when applying the
\verb!take 1! function as we do not attempt to evaluate them.

A second characteristic often associated with lazy evaluation is that of 
updating. When an expression is evaluated in a lazy context, and we know
that the contents of the expression will not change, we can overwrite the
expression with its final value~\cite[pp.208]{SPJ}. Implementation wise, this generally involves
replacing the pointer we previously associated with the expression in the
heap with a pointer to an indirection node pointing to the result of 
evaluating the expression. From then on, if our program attempts to 
access the expression, it will be redirected to a value in the heap 
representing the evaluated expression. This means we need only evaluate
the expression once. This is only possible in cases where
we can say that the expression in question is \emph{referentially transparency}. 
Referential transparency is the name
given to a property of certain expressions which states that the expression
can be replaced with its value without altering the semantics of the overall
program. This property exists in programs which are \emph{pure}, that is to
say programs consisting of expressions or functions whose evaluation is 
dependent only on the body of the expression and it's parameters, and which
cause no side effects which may effect other expressions upon execution. 
We cannot update an expression with its value if there is a danger that
some aspect of the expression may change during the course of program 
execution. This creates certain challenges when dealing with non-deterministic
concepts such as IO in functional languages. Such problems however are 
beyond the scope of this project and implementation.

\subsection{Weak Head / Normal Form}
\emph{Normal form}, and \emph{weak head normal form} or WHNF, are terms used
in this context to denote the level of possible evaluation in an expression.
We say an expression is in head normal form when it has been fully evaluated
or reduced, and can neither be further evaluated nor contains any sub
expressions which can be further evaluated. We'll examine the following examples.

\begin{itemize}
 \item 1
 \item \verb!\x -> x!
 \item \verb!(1, 2)!
\end{itemize}

These are an integer, lambda expression and tuple respectively in normal form. \verb!1 + 1!
by comparison would not be in normal form as there is an addition operation to
be performed.

Expressions in weak head normal form are expressions that have been evaluated
to the outermost data constructor~\cite[pp.198]{SPJ}. These can be data constructors, under-saturated
primitive operations or lambda abstractions. For example...
\begin{itemize}
	\item \verb!Just (1 + 1)!
	\item \verb!\x -> 2 + 2!
	\item \verb!'h' : ("e" ++ "ello")!
\end{itemize}
The first example contains two sub expressions which could be further evaluated,
but as the outermost component (the Just data constructor) has been evaluated 
fully, it is still in WHNF. The same is true of the 3rd example, where the "++"
sub expression could be further evaluated, however the outermost element
':' is a data constructor, specifically the list cons constructor. The second
example is in WHNF by virtue of being a lambda abstraction.

