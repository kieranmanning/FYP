
\section{A High Level Design}
\subsection{Overview}
Having looked at the problems we wish to solve and some existing solutions, we 
now turn our attention to the implementation of our own solution. First,
we form an image of the overall architecture of our solution. At a
very abstract, high level we can say our project will require the following:

\begin{enumerate}
	\item An input language rich enough to allow us to write programs
		  that capture our intended semantics and features.
	\item A transformation from this language into the actions we 
	 	  wish to be executed by a browser.
\end{enumerate}

\noindent The latter requirement can be decomposed into a number of sub-requirements.
We will need to split our transformation firstly into compilation and runtime phases.
A source program written in our input language will first be compiled into some runnable
representation thereof by a compilation stage. This representation of our program will
then be executed by our runtime until a value is returned. We can be somewhat vague
for the moment about the exact details of the input language and compilation phase, 
but we do know that our runtime must be capable of executing programs in a web browser.
We could say trivially that we only need to compile our programs in advance to simple
end values which could be interpreted by a browser, but this would leave us with a 
glorified static expression evaluator. Thus, our runtime must be executable from a
web browser. With this in mind, we can say that it is the job of our compiler to
take our input language and transform it into a representation which our runtime can
then execute when called from a browser. 

Seperating our compiler and runtime poses a small problem. We now have two disparate
stages which will need to communicate in some sense. We could write both our compiler
and runtime in a language which can be executed by a browser, but this would be 
unnecessary and an inefficient use of any such language. Instead, we should write our
compiler in a more general purpose language and find a means of converting our compiled
representation into one which can be understood by our runtime. Let us now re-examine
our project requirements.

\begin{enumerate}
	\item An input language rich enough to allow us to write programs
		  that capture our intended semantics and features.
	\item A compiler capable of transforming our input language into
		  a program representation executable by our runtime.
	\item A means of converting our compiled representation into one
		  which can be executed by our runtime.
	\item A runtime to execute our program in a browser. 
\end{enumerate}

\subsection{Input Language}

**Note we haven't commented on the format of core programs beyond **\\
**recognizing them at a glance. Need to go into detail on [ScDefn]**\\
**and others, although we could probably leave that until impl	  **\\

\noindent We will start by examining the first requirement. We will need a
language that is capable of conveying at the very least the generic basics 
of a programming language, namely the notions of primitive value and function
declarations and function applications. After this, our language will need
some means of flow control, namely conditionally executed statements and 
loops. The last of these basic required features is some manner of stuctured
data type, allowing for the creation of more complex types as composites of
primitive types. 

Our resultant compiled language will need to type-safe and support lazy
evaluation. The former will require some variety of type-checking to be 
performed on our input language at some stage during compilation, and any
type errors found to be dealt with and report. The latter is more of a 
compilation concern than one one of language choice, although a pure input
language which we can guarantee side-effect free would be of great help.

At this point, we can start to see certain parallels with our ideal input
language and currently existing programming languages. What we have described
looks a lot like the generic template for functional languages such as Miranda
or Haskell, with a very simple feature set. It could also be compared to 
classic LISP with the addition of structured data types. 

On a more interesting note is the similarity of our required language to
certain subsets of Lambda Calculus. Before examing these subsets, we'll
take a quick look at some simpler dialects. The basic
features of simple untyped lambda calculus are lambda terms, denoting well 
formed expressions in the lambda calculus which can consist of:

\begin{itemize}
\item Variables
\item Lambda abstractions
\item Lambda applications
\end{itemize}

\noindent where the latter two correspond with function abstractions (or
definitions) and applications in more familar terminology. The lambda term
representing a function that takes two parameters and returns their sum 
would be 

\begin{center} 
	\( \lambda x y \rightarrow x + y \)
\end{center}

\noindent The typed varieties of lambda calculus add the concepts of types and type
notations, updating the syntax with a new construct \(x:\Gamma \) indicating
a variable \(x\) of type \(\Gamma\). This would be the only significant 
difference in the simply typed dialect. 

Let us now look at a variant of the language known as System F. This dialect
adds the notion of type polymorphism or universal type quantification to 
the simply typed lambda calculus. The effect of this is to allow for the
use of variables which range over types as well as functions, in comparison
to the symply typed lambda calculus where variables only range over functions.
In practice this allows us to reason about the types of functions, which also
provides us with the ability to write functions which range over universally
quantified types. As an example, we can express the identity function as

\begin{center}
 \( \Lambda \alpha . \lambda x^\alpha . x : \forall \alpha . \alpha \rightarrow \alpha  \)
\end{center}

\noindent which we read as \emph{the type level function which takes 
the type \(\alpha\) and returns the id function \( \lambda x^\alpha . x \)
(which is of type \emph{ function that takes a paramater of type \( \alpha \) 
and forall type \(\alpha\) returns a value of type \( \alpha \)})}, where 
the id function can be read as \emph{the function that takes an x of 
type \( \alpha \) and returns same}. Those interested will find further information
on System F here LINKLINK

The reason System F is so intereesting to us is its similarity to a number of functional
language representations currently in use. Many high-level functional programming languages
can be expressed in terms of System F while still preserving their full semantics. Of
particular note however are the languages of Core and SAPL. These are intermediate languages
used as minimal representations for Haskell and Clean respectively which can be emitted by
specific compilers for each language midway through compilation. The term 'Core' is often
used to refer to any such intermediate functional language based on typed lambda calculus,
but from here on we shall refer to the GHC (Glasgow Haskell Compiler) specific intermediate
language as Core or 'External Core' and similar languages as 'Core-like'. Core is a lambda
calculus dialect (System F with added type coercions to be specific~\cite{SystemF}) which
can be obtained from GHC with the compiler argument -fext-core. SAPL can be similarly 
obtained from the Clean compiler. 


\noindent The semantics of Core look like this~\cite{Ext-Core}..

\begin{verbatim}
type CoreExpr = Expr Var

data Expr b 
  = Var	  Id
  | Lit   Literal
  | App   (Expr b) (Arg b)
  | Lam   b (Expr b)
  | Let   (Bind b) (Expr b)
  | Case  (Expr b) b Type [Alt b]
  | Cast  (Expr b) Coercion
  | Note  Note (Expr b)
  | Type  Type
\end{verbatim}

\noindent ...when expressed in terms of a Haskell algebraic data type (which 
coincidentally is a nice way to express most things in life). Lam in this
case stands for Lambda and the liberally added 'b's for the types of the binders
in the expression, the rest is mostly self explanatory. Note that data constructors
are not represented here although are included in the language seperate to the
Core expr type, along with a \%data denotation.

By the time a source program reaches the stage of GHC where it can be emitted
as Core, it has already undergone type checking. It has also been substantially 
minimized to the simplest representation
possible still perserving all of the original program's semantics. This makes
Core an excellent candidate as an input language, as its minimalism makes it
easy to parse and allows us to make assumptions about its type safety.

To give an idea of what a Core program looks like, we'll examine a small 
example. A factorial program is the canonical functional Hello World and will
demonstrate a few important concepts, so lets go with that. Our source program
will be 

\begin{verbatim}
module MIdent where

fac :: Int -> Int
fac 0 = 1
fac x = x * (fac x - 1)

main = fac 4
\end{verbatim}

\noindent Compiling this with ghc -fext-core produces the following output...

\begin{verbatim}
%module main:MIdent
  %rec
  {main:MIdent.fac :: ghczmprim:GHCziTypes.Int ->
                      ghczmprim:GHCziTypes.Int =
     \ (dsdl0::ghczmprim:GHCziTypes.Int) ->
      %case ghczmprim:GHCziTypes.Int dsdl0
      %of (wildX4::ghczmprim:GHCziTypes.Int)
        {ghczmprim:GHCziTypes.Izh (ds1dl1::ghczmprim:GHCziPrim.Intzh) ->
           %case ghczmprim:GHCziTypes.Int ds1dl1
           %of (ds2Xl6::ghczmprim:GHCziPrim.Intzh)
             {%_ ->
                base:GHCziNum.zt @ ghczmprim:GHCziTypes.Int 
                base:GHCziNum.zdfNumInt wildX4
                (base:GHCziNum.zm @ ghczmprim:GHCziTypes.Int
                 base:GHCziNum.zdfNumInt (main:MIdent.fac wildX4)
                (ghczmprim:GHCziTypes.Izh (1::ghczmprim:GHCziPrim.Intzh)));
                (0::ghczmprim:GHCziPrim.Intzh) ->
                 ghczmprim:GHCziTypes.Izh (1::ghczmprim:GHCziPrim.Intzh)}}};
  main:MIdent.main :: ghczmprim:GHCziTypes.Int =
    main:MIdent.fac
    (ghczmprim:GHCziTypes.Izh (4::ghczmprim:GHCziPrim.Intzh));
\end{verbatim}

\noindent Which is intended to be partially human readable, but mostly
just parseable. Lets tidy this up a bit...

\begin{verbatim}
%module main:MIdent
%rec
{fac :: Int -> Int =
\ (d0::Int) -> %case Int d0 %of (wildX4::Int)

{Types.Izh (d1::Int) -> %case Types.Int d1 %of (d6::Int)

{%_      ->
    * @ wildX4 (- @ (fac wildX4) 1::Int);

(0::Int) ->
    (1::Int)}}};

main :: Types.Int = fac (Types.Izh (4::Int));
\end{verbatim}

\noindent That looks nicer. Here, I have shortened the type names in the
primitive value type notations, removed the function return type notations
and replaced the longwinded arithmetic operator names with their traditional
symbols. We can see that the guards (Haskell pattern matching syntax, denoted
with '\textbar' symbols) have been removed and replaced with a case statement, in
keeping with Core's minimal mindset. We can also see a lambda calculus esque
structure, with our computations being represented in the form of \( \backslash
 [params] \rightarrow function body\). The influence System F can be seen in
 the type signatures prefixing the fac function. Of note also is the '@' symbol, used
 to denote function application. More on this later, for the moment it's enough
 to know that x + y can be represented as + @ x y, that is \emph{plus applied to
 x and y}.
 
SAPL is similar to core, with a few differences. SAPL uses a \emph{select} 
expression in place of Core's case expressions although this is a difference in name
only, the semantics of both being equivalent. SAPL also has a built in \emph{if}
expression which in Core would be expressed using case expressions instead. SAPL
contains no type annotations, all such information having been removed after the
Clean compilation type-checking stage.

Choosing a language such as Core or SAPL as our input language would provide
a number of benefits for this project. Firstly, it would allow us to write
programs in Haskell or Clean and make use of existing tools to translate these
into their Core-like representations. This would mean that we can write our browser
programs in languages with pleasant high level syntax. We could leave the 
parsing of these programs to existing Haskell or Clean compilers and focus 
instead on the more interesting problems of compilation and runtime. We would
also get type-checking for free, as the type checking for both Core and SAPL 
would have already been performed by the time they are emitted by their respective
compilers. It would also reduce the learning curve for users who wish to use
this project who already have experience with writing programs in languages
with syntax similar to Haskell or Clean.
 
Having seen the various features of System F languages such as SAPL and Core, I
was quite content to choose such a language as the input for my own project. 
From our research into the iTasks system, we have seen that such a language 
can provide a good starting point for a project such as this. Between SAPL and
Core I was more inclined towards using the latter. I have far more experience
using Haskell than Clean. Also, though we do not need them currently, it is
worth noting that having Core's type annotations may prove useful for future
improvements to our project. Such information would be of great help when 
diagnosing runtime errors, for example. It was decided that Core would be the
input language.

\subsection{Compilation Strategy}
Now that we have decided on our input language, we need to plan our compilation
strategy. Thankfully for us, there is a wealth of literature and previous work
on writing a compiler for System F resemblant languages and a number of approaches
already exist from which we can take inspiration. An overview of general 
functional language compilation is provided in the background chapter of this
report so that this section and following sections discussing compilation can 
focus on the aspects of immediate interest to us.

*ADD GRAPH REDUCTION TO BACKGROUND*

First, we need some way to compile programs written in our input language into
some form from which we can derive an initial state. The traditional way to
do this is to view our program as a graph as described above and convert this
graph into a programmatic representation. There are many existing functional
compilation language strategies which achieve this, so it was decided to 
examine those before deciding on an implementation. In particular, there were
two strategies which I investigated in-depth

\begin{enumerate}
	\item The template instantation machine
	\item The G-Machine
\end{enumerate}

\subsubsection{The template instantiation machine} 
The template instantiation machine is a simplistic approach to
functional graph reduction and compilation but conveys many of 
the ideas used in more complicated approaches. When the project
started moving towards implemenation of a solution, this is the
approach I started with. It proved very useful in understanding
some of the key ideas in writing a functional compiler and gave
me an idea of the scope of such an implementation. 

The machine is built upon a state consisting of
\begin{itemize}
	\item A stack
	\item A dump
	\item A heap
	\item A globals array
\end{itemize}

\noindent The stack is a stack of addresses which correspond
to nodes in the heap. These nodes form the spine of the 
expression being evaluated. The dump is used to record the
state of the expression being evaluated. When we **EXPLAIN**.
The heap is a list of nodes and their associated heap addresses.
The nodes contained in the heap correspond to the examples
given in the overview in the background section.
The globals array associates the names of our declared
supercombinators with their addresses in the heap, allowing
supercombinators to be looked up by name. 

The operation of the template instiation machine is described
by a number of state transitions which are called if the state
of the machine represents that of the transition. When no
transition rule matches, we assume execution to be complete. 
At most one state transition rule will apply at any point;
more than one would imply non-determinism which is not
permissible. In general, the state transition rules are
concerned with the type of node pointed to by the top of the
stack and will react accordingly. For example, in the case
where an application node is found, the appropriate rule
is triggered which will pop the application and unwind its
two arguments onto the stack. The rule for dealing with 
supercombinators is also of interest to us. When a 
supercombinator node pointer is found on top of the stack
we instantiate it by binding the argument names of its
body to the argument addresses found on top of the stack.
We then discard the arguments from the stack, reduce the
resulting redex and either push the root of the result 
to the stack or overwrite the root of the redex with the
result, depending on whether the implementation in question
is performing lazy updates. 

The implemenation of the template instantiation machine is
split into compilation and evaluation stages. The compiler
in the first stage takes our input program, along with
any built-in prelude definitions, and builds an initial
state of the form described above. The evaluator in the 
second stage takes this initial state and runs our machine
on it one step at a time, applying the necessary state
transition rules, until we reach a final state. The stepping
function takes as input a state and returns a resultant
state. The machine is considered to have finished when 
the stack consists of a single pointer to an item which
can be evaluated no further, for example and integer or
a fully saturated data constructor. 

**PROVIDE EXAMPLE COMPILATION**

This is about the extent to which I implemented my initial
template instantiation machine. There were further 
improvements which could be made, such as adding primitive
operations, let expressions etc. etc. My implementation was
capable of taking a program representing an identity 
function, compiling it and evaluating it. This was 
sufficient for my purposes of understanding some of the
basic concepts of functional compiler implementation. At
this point, I was looking to move towards a different
implementation which would better suit some of my 
requirements. Those interested will find a more complete
implementation of a template instantiation compiler 
described in greather depth in **SJP BOOK AND TUTORIAL**.

The template instantiation machine suffers from one notable
problem which made it particularly unsuitable for our
purposes. The general operation of the template instantiation
machine constructs an instance of a supercombinator body.
Each time we attempt to instantiate a supercombinator we
recurseivly traverse the template. This action is executed
in the evaluation stage. We know that our end goal is the
ability to execute programs in a browser and as such it would
be of great benefit to us if we could minimize the amount of
work needed at run time. Thankfully for us, there exist
implemenations which are capable of doing this!

\subsubsection{The G-machine}
The G-machine differs from the template instantiation machine
in a few ways but there is one significant difference in 
their principles of operation. The G-machine translates each
supercombinator body to a sequence of instructions which will
construct an instance of the supercombinator body when run.
In comparison to the template instantiation machine, this
allows us to execute the actions of a supercombinator without
the need to instantiate it at run time, this having been
achieved in advance at compile time. 

The G-machine decouples the compiler from evaluator to a
greater extent than the template instantion machine. The
G-machine's compiler not only produces an initial state from
our input program but also a list of instructions which when
combined with the initial state can be evaluated to a final
state. The set of instructions drawn from is designed to be
minimal. The instructions and state produced
by the compiler can be said to represent an \emph{abstract
machine} which can then be implemented on various different
architectures and platforms. This makes it easier to write
runtime evaluators for G-machine compiled programs, as the
initial state can be expressed in an intermediate form 
between that of a reducible graph state representation 
and an easily executed program state. This fact will prove
useful when it comes to writing the runtime to evaluate
our compiled programs.

The general form of the G-machine is similar to that of
the template instantiation machine. We take our input language
and compile this into an initial state. This state is identical
to the one listed previously, with the addition of a sequence
of instructions to be called in order to evaluate our state.
There still exists a heap and stack, the former still containing
nodes of the same types as those previously encountered.
This state is passed to the runtime which will execute until
we reach a final state where we have no further instructions
to evaluate and a single pointer to a node in the heap on top
of the stack. Our runtime must now possess the means to 
interpret these instructions and execute them upon the current
state, producing a new state. This is in contrast to the 
evaluation stage of the template instantiation machine which
had a list of state transition rules which would fire when
the state matched that of one of the rules. Here, only the
initial state was necessary for evaluation.

The following are instructions one would find in a
basic G-machine implementation. 

\begin{itemize}
	\item Unwind: unwinds the spine in much the usual
		  manner.
	\item PushGlobal: finds a supercombinator by name 
		  from the globals array and places its heap
		  address on top of the stack.
	\item PushInt: Places an integer node in the heap
		  and its address on top of the stack.
	\item Mkap: Forms an application node from the
		  top two node pointers on the stack.
	\item Slide: drops N pointers from the stack behind
		  the current top pointer. Used to remove
		  arguments pointers after evaluating a 
		  supercombinator.
	\item Update: updates the root of a redex with its
		  reduced value. Used to implement laziness.
\end{itemize}

\noindent More in-depth information on these and other 
instructions can be found in later implementation chapters. 

The G-machine compiler consists of a number of schemes which
determine what sequence of instructions to output when we
encounter certain expressions in our input language. These
together with an initial heap built from the supercombinators
of our input program along with any prelude definitions are
then passed to the runtime.

The improved efficiency of the G-machine over the template
instantion machine, along with the intended ease of writing
an abstract machine to evaluate programs at runtime, makes
it the better choice of the two explored implementations.


\subsection{Runtime}
Now that we have decided on the G-machine as the general
form of our compilation strategy, we need to consider how
we will evaluate G-machine compiled programs in a manner
that is useful in web programming. As anticipated from
early on in this project, JavaScript is the only realistic
way to achieve this. JavaScript is far and away the most
commonly used browser-side programming language. If we
want the results of this project to be usable in any
general sense, JavaScript is the only sensible option.

It is worth re-examining Fay at this point. We are
taking some inspiration from its implementation in
that we are using a runtime written in javascript to 
evaluator a low level representation of our compiled
program. However, it's worth noting the differences.
As we have decided to use the G-machine as our overall
compilation strategy, we 
will not need to concern ourselves with the
concept of thunks. Our smallest indivisible unit of
computation will instead be singular graph nodes. 
In a sense, these are comparable to the thunks
represented in Fay placed into the context of a graph
reduction machine. Our primitive operation instructions
will be implemented as operations on graph states
returning new graph states, as opposed to thunks. We
do not concern ourselves with forcing thunks, so to
speak, but we will have to deal with evaluating 
computations to weak head normal form, albeit in a 
graph reduction context.

Our runtime will receive the graph state representation
of our input program after compilation, along with the
instruction sequence associated with this state. It will
have to iterate through the instruction sequence, enacting
the actions specified by the instructions upon our state.
We can see that our runtime will need to understand how
to interpret these instructions into concrete javascript
actions which we can then call from our browser. Our 
runtime must also be capable of representing our compiled
initial state and instructions in a javascript format 
for our actions to be enacted upon. We will 
need some abstract representations of the components of
our state, our instructions and their attributes. The
compiled state will need to be serialized from the 
language which we use to represent our program through
the compilation stage (which as it turns out will be
Haskell) into Javascript. This will be achieved by
instantiating our abstract state javascript
representations to represent the concrete components of
our state and instructions.

At this point, we will have a compiled G-machine state 
in Javascript form and the means to evaluate it into
subsequent states. 
A javascript function will iterate through our states
appling the changes necessary. On each iteration, this
will check the state of the stack and
the instruction at the front of the instruction list,
apply the evaluation actions required by the instruction
upon the state and return a new state. This will need 
to be achieved for each state and instruction in our
evaluation until we reach a final state (which we will
also need a means of testing for). A call to this
function will be included in our serialized compiled
state which we can call to initiate execution.
When we have finished executing, we will then need
to extract the result of our evaluated program from
our state and return it in a javascript format for 
use in our browser, other javascript functions etc.

\subsection{Summary}
We have decided that our project will consist of a Gmachine
based compiler that accepts GHC's external Core. This 
will be compiled into a graph based representation of the
initial state of our program. This initial state will be
serialized to a JavaScript representation thereof. A 
Gmachine graph evaluator implemented in JavaScript will
then evaluate this initial state to a final state and
resultant value
