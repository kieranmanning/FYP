\documentclass[11pt]{article}
%Gummi|062|=)

\usepackage{fancyhdr}
\usepackage{verbatim}

\title{\textbf{FYP(!)}}
\author{Kieran Manning \\ 09676121}
\date{December 2012}

\newcommand{\lang}{\$LANG}

\begin{document}

\maketitle

%	i, ii etc. page numbering for contents, intro etc. etc.

\pagenumbering{roman}

\tableofcontents

\newpage

\setcounter{page}{1}

\section{Acknowledgements}
Lorem ipsum

\section{Abstract: a short summary} 
 This project explores the use of JavaScript as a target language for a Haskell EDSL. Program development in JavaScript is difficult due to various weaknesses in the language design. Rather than attempting to fix the flaws in the language we will explore the practicality of using JavaScript as a target language for developers who wish to work in the higher-level Haskell language. Rather than translating Haskell programs to JavaScript we provide an interpreter which can execute Haskell programs that have been translated to a core language; by taking this approach we are able to discuss the preservation of the program semantics with more confidence than in a direct-translation approach.

\pagenumbering{arabic} 

\newpage

\section{Goal}
The goal of this project is try and improve on some of the problems inherent in writing
programs directly in javascript by providing a secondary language into which javascript
can be compiled. Javascript can produce perfectly acceptable client-side browser-executed
web programs. In practice however, it is difficult to write Javascript which produces 
such perfect results. Javascript suffers from a number of problems. It has a very weak, 
dynamic type system which results in a lot of reliance on type coercion and programmer
sanity. Unpredictable results which arise through this weak typing are frequent causes
of failures and bugs in javascript programs, leading to its notoriety as a difficult 
language to write safe programs in, especially for programmers new to the language.

The syntax of javascript is verbose and arguably unpleasant, the product of a different
age of language design. As such, learning to use javascript can be quite painful, tedious
and counter-intuitive for the first-time programmer. This problem persists even for
more experienced users, in the difficulty inherent in trying to find bugs or errors in
failing javascript or attempting to become familiar with an existing javascript codebase.
A number of other languages have been created in an attempt to solve this problem, a prime
example being CoffeeScript, which is discussed further in section 5.4. The product of this
project should aim to provide a more user-friendly syntax, inspired by the improvements
in language design since the introduction of javascript and, which will be easier
to write and read for experienced and beginning programmers alike. 

We would also hope to introduce the notion of lazy evaluation into client-side programming
with javascript in an accessible, straight forward manner. It is possible to write programs
in a lazy style with javascript by viewing all evaluations as 'thunks' represented by 
javascript functions, but in practice this is an uncommon approach. Achieving this, however,
requires what is essentialy creating a lazy domain specific language in Javascript and 
adapting your programs around this. Libraries such as stream.js have tried to achieve this
in part. Lucky for us, adding such functionality becomes substantially easier when using
javascript as a target language, allowing us to view javascript and a lazy DSl as the 
'runtime' for our language. 

If we are going to the trouble of implementing all of this using haskell and GHC it would
be nice to get some benefit from our work! That is why I've opted to use GHC's Core 
intermediary language as an intermediary in our own language. By the time a program reaches
the Core 'stage' of its compilation, GHC has already checked its validity at the type level.
By using core as our intermediary, we can take advantage of the type system and optimizations
already implemented in GHC to do some of the heavy lifting for us. 

\pagebreak

\section{Introduction}
It would help to explain some of the concepts to which I will be referring 
throughout this report. Not everyone will be familiar with some of the 
functional programmming, and more specificially Haskell, ideas being discussed.
While many programmers will have an idea of the workings of javascript, at
least in so far as it is an imperative language, some of the concepts which 
we will be examining might go beyond the scope of casual javascript programming.
For brevity's sake, I will explain some of the more specific concepts which 
I will be commonly referring to throughout this report.

\subsection{Types in Javascript}
There are seven data types in javascript, five of which concern us for the purposes
of this project; three primary data types which are Number, String and Boolean, 
and two 'composite' data types, Objects and Arrays. When writing in straight
forward javascript, types are more or less invisible to the programmer. To declare
a variable, of any type, the usual format is...
\begin{verbatim}
var varname = value;
\end{verbatim}
The type of varname is inferred from the type of the value. Types can be mixed
in operators and functions with some type coercion, which can be very dangerous
if used incorrectly. The composite Object type will be most interest to us later,
when dealing with the javascript 'runtime' used both in our own implementation
and in that of the Fay language, which we will be looking at.

An important point to remember is that we can represent primitive objects as
composite objects, albeit it with a performance penalty. This idea will prove
significant later when dealing with lazy evaluation, where simple primitive
types will be insufficient to represent lazy evaluation.

\subsection{Functional Languages}

\subsection{Functional Compilation}
I will be referring frequently to some of the more common ideas and practices
invovled in compiling a functional language thoughout this report. Unsurprisingly,
the field of functional compiler design is not easily summarized in a paragraph,
let alone a paper; the implementation explained here only scratches the surface.
However, it will be easier to understand some of the points I make if we start
with a quick overview.

Most compiler implementations for simple functional languages start with an 
input language which consists of a list of function definitions and some entry
or "main" function, which serves as the entry point for the runtime or evaluator.
We wish to avoid writing programs with side effects and as such the syntax of
our input program will reflect that. There is no concept of global state nor
stateful computations and when we need to represent some concept of changing
state we do so by passing state as paramaters between functions. 

The input language is parsed into a simplified expression-based representation.
One such representation, the one used later in this report, is defined as:

\begin{verbatim}
Program ::= [EVar, [a], Expr a]

Expr a
  ::= EVar Name
    | ENum Int
    | EConstr Int Int
    | EAp (Expr a) (Expr a)
    | ELet 
        IsRec
        [(a, Expr a)]
        (Expr a)
    | ECase
        (Expr a)
        [Alter a]
    | ELam [a] (Expr a)
\end{verbatim}

\noindent We say that a program in this representation consists of a list of
super combinator definitions. A super combinator, for our purposes, is an 
expression body along with an identifying variable name and a list of arguments
to the expressions. Expressions can consist of

\begin{enumerate}
	\item EVars. Variable names corresponding to a defined value.
	\item ENums. Primitive integers
	\item EConstrs. Data constructors, along with the number of elements in the
		  fully saturated constructor as well as the constructors tag, used to
		  differentiate constructors in a given context.
	\item EAp. Binary function applications which we use to apply exprs in order
		  to construct larger exprs.
	\item ELet. Let expressions, allowins us to define a list of arguments local
		  to a specific expression.
	\item ECase. Case expressions, containing first a condition expression which
		  will evaluate to a tag identifying a data constructor in the context
		  of the case expressions and secondly, a list of constructor tags and 
		  the code to be executed when a particular tag is found.
	\item ELam. Lambdas, or anonymous expressions. Note the similarity to a 
		  super combinator, without the identifying variable name.
\end{enumerate}

\noindent By way of example, a sample program taking two paramaeters and returning
their sum, written in such a language, would resemble

\begin{center} \( ["sum"], ["x", "y"], EAp (EAp (EVar "+") (EVar "x")) (EVar "y") \) \end{center}

Note in particular that we address a supercombinator named "+" 
representing our integer addition primitive and that we use two EAp 
binary expression applications in order to call it on two parameters x and y.

Our compiler will take a program in such a simplified representation
and convert it into an initial state, along with an initial sequence of
instructions required to evaluate the initial state. An interesting 
distinction between these compilation schemes and those of imperative languages, 
is that we can view our initial (and continuing) program states as graphs built
from a small set of nodes, namely

\begin{enumerate}
	\item Integer nodes, representing literal integer values
	\item Function nodes consisting of the arity of the function and the
		  its behaviour along with the instructions needed to evaluate it.
	\item Application functions, the only nodes capable of forming 
		  node connections from a graph point of view and used to represent
		  expression applications in terms of our simplified expr language.
	\item Data Constructor nodes, consisting of the arity of the data
		  constructor and a list of addresses to the nodes containing its
		  elements.
\end{enumerate}

Our initial program state will consist of a heap containing a list of these
nodes and their attributes addressable by their index in the heap. We will
also have a stack, used to contain a working set of heap addresses which 
will be required in the evaluation of immediately pending expressions. A 
'globals' dictionary will associate global function identifiers with the
addresses of their function nodes in the heap. **GIVE EXAMPLE OF GRAPH**

**EXAMPLE OF REDEX REDUCTION IN RELATION TO GRAPH REDUCTION**

An evaulator or runtime evaluates our state by iterating through the 
sequence of instructions provided, updating the state as necessary until
some final state is reached (or something breaks). This evaulator will 
understand the actions to inact on our state upon finding a given instruction
in the instruction sequence. We can break down our instructions into two
types; those which perform janitorial work on our state and those which
progress the evaluation of our input program towards some final state. 
Upon encountering an instruction of the latter type, our evaluator will 
in general push the heap addresses for the parameters requested (if any) 
to the stack, execute the actions required by instruction to the parameters
pointed to by the top x addresses on the stack and the result value in 
the heap and its heap address to the top of the stack. **EXAMPLE**. We
continue evaluating until we have reached the end of the code sequence
or until something breaks. 

\subsection{Lazy Evaluation}
Lazy evaluation, also known as call-by-need evaluation, is an evaluation method
that delays evaluations until such time as their end values are directly
required. The result of this is that a computation we write will only be 
evaluated when it is required by some other aspect of the program, or possibly never
if its value is never required. 

This is useful both from an efficiency point of view and when trying to 
represent concepts which might not fit into stricter, more finite ideas of
programming. 

For the former, it is easiest to demonstrate with an example...
\begin{verbatim}
take 1 ['a'..'e']
\end{verbatim}
Here, \emph{['a'..'z']} represents a list of characters beginning with 'a' and 
ending with 'e'. The function \emph{take} evaluates and returns the number of
elements of the second parameter specified by the first parameter, starting at the
head of the list. In this example we're 'taking' one element from "abcde" which
of course gives us the character 'a'. But what if we were to say...
\begin{verbatim}
take 1 [1..]
\end{verbatim}
As you may have guessed, \emph{[1..]} is the list of integers beginning with 1
and continuing to infinite. Strange concept? In strict evaluation, attempting
to make use of this would be difficult, to say the least. However, with lazy 
evaluation we only need to evaluate as much of the list as we actually need.
The result being that our take function will only evaluate and return the 
first item, ignoring the remainder of the list. What's more, it can do this
as quickly with an infinite list as with any other length list. This is where the
efficiency bonus comes in! **Is that right?**. \\


**EXPLAIN IMPLEMENTATION RE: OUR GRAPH REDUCTION**  \\


A second characteristic often associated with lazy evaluation is that of 
updating. When an expression is evaluated in a lazy context, and we know
that the contents of the expression will not change, we can overwrite the
expression with its final value. Implementation wise, this generally involves
replacing the pointer we previously associated with the expression in the
heap with a pointer to an indirection node pointing to the result of 
evaluating the expression. From then on, if our program attempts to 
access the expression, it will be redirected to a value in the heap 
representing the evaluated expression. This means we need only evaluate
the expression once. 

This chractecteristic, known as sharing, is closely related to the idea
of \emph{referential transparency}. Referential transparency is the name
given to a property of certain expressions which states that the expression
can be replaced with its value without altering the semantics of the overall
program. This property exists in programs which are \emph{pure}, that is to
say programs consisting of expressions or functions whose evaluation is 
dependant only on the body of the expression and it's parameters, and which
cause no side effects which may effect other expressions upon execution. 
We cannot update an expression with its value if there is a danger that
some aspect of the expression may change during the course of program 
execution. This creates certain challenges when dealing with non-deterministic
concepts such as IO in functional languages. Such problems however are 
beyond the scope of this project and implementation. Those interested can
find more information here **LINK**

\subsection{Weak Head / Normal Form}
\emph{Normal form}, and \emph{weak head normal form} or WHNF, are terms used
in this context to denote the level of possible evaluation in an expression.
We say an expression is in head normal form when it has been fully evaluated
or reduced, and can neither be further evaluated nor contains any sub
expressions which can be further evaluated. For example...
\begin{verbatim}
1
\x -> x
(1, 2)
\end{verbatim}
are an integer, lambda expression and tuple respectively in normal form. "1 + 1"
by comparison would not be in normal form as there is an addition operation to
be performed.

Expressions in weak head normal form are expressions that have been evaluated
to the outermost data constructor. These can be data constructors, undersaturated
primitive operations or lambda abstractions. For example...
\begin{verbatim}
Just (1 + 1)
\x -> 2 + 2
'h' : ("e" ++ "ello")
\end{verbatim}
The first example contains two sub expressions which could be further evaluated,
but as the outermost component (the Just data constructor) has been evaluated 
fully, it is still in WHNF. The same is true of the 3rd example, where the "++"
sub expression could be further evaluated, however the outermost element
':' is a data constructor, specifically the list cons constructor. The second
example is in WHNF by virtue of being a lambda abstraction.


**REDEX EXAMPLE**

\section{Problem Space}

\subsection{Syntax}
JS syntax is hella pants. Able to fix this though!
\begin{itemize}
\item Generally awkward, verbose
\item JS is Old. New ideas on syntax have emerged
\item Succinct simplicity of syntax in languages such as Python as contrast
\item Overview of ease of writing DSLs in haskell as a solution
\end{itemize}


\subsection{Weak Typing}
\begin{itemize}
\item Dangers of type coercion 7 + 7 + "7"; // = 147  "7" + 7 + 7; // = 777 . 
	  Common complaint and cause of errors, much like PHP.
\item Dynamic (runtime) vs. static (compiletime). Discussion on HindleyMilner.
\item How much of this we can represent in Lambda Calculus/SystemF
\item What do we lose by going static?
\end{itemize}



\subsection{Late Binding}
Efficiency, runtime vs. compile time typing

\pagebreak

\section{Existing Solutions}
I am not the first person to examine the possibilities of client-side web
programming with Haskell.  

\subsection{Fay}
The Fay language, which lives at https://github.com/faylang/fay/wiki, is
another language which has attempted to solve some of the problems we
are interested in. It provides a Haskell subset DSL to programmers, taking
advantage of existing Haskell syntax. The haskell FFI, or Foreign Function
Interface is then used to connect this to a 'runtime' written in javascript.
This runtime consists of javascript functions representing functional and
lazy computations ie. thunks and variably saturated functions. This allows
Fay to translate Haskell programs into Javascript while preserving notions
such as laziness and purity, while also making use of Haskell's type system
resulting in a strongly typed language.

Javascript types are represented in the DSL through the use of 'Fay' types,
for example...
\begin{verbatim}
getEventMouseButton :: Event -> Fay Int
getEventMouseButton = ffi "%1['button']"

focusElement :: Element -> Fay ()
focusElement = ffi "%1.focus()"
\end{verbatim}
Here, Fay Int is a type defined in the Fay subset which represents a javascript
primitive integer. This will be compiled into an equivalent javascript function. 
Using a Haskell DSL allows Fay to make use of Haskell's type system and strong
typing to produce equivalently safe code in Javascript. It also allows the
programmer to benefit from many of the benefits that come with programming with
Haskell, type signatures being of particular note in this example. The second
snippet is included to show an example of representing a non-returning function
in Fay. FocusElement will take an Element type and return no value but will
execute a Fay action, which will correspond to some javascript function.

Fay's javascript 'runtime' uses functions to represent objects. In this manner,
the idea of \emph{first class functions} can be preserved from Haskell in the
translation to Javascript. A 'thunk' for example looks like this...
\begin{verbatim}
// Thunk object.
function $(value){
  this.forced = false;
  this.value = value;
}
\end{verbatim}
We can see Fay making use of javascript's concept of all-encompassing objects to
make representing functional code easier. 

\begin{verbatim}
function Fay$$mult(x){
  return function(y){
    return new $(function(){
      return _(x) * _(y);
    });
  };
}

function Fay$$mult$36$uncurried(x,y){
    return new $(function(){
      return _(x) * _(y);
    });
}
\end{verbatim}



There are seven data types in javascript, five of which concern us for the purposes
of this project; three primary data types which are Number, String and Boolean, 
and two 'composite' data types, Objects and Arrays. When writing in straight
forward javascript, types are more or less invisible to the programmer. To declare
a variable, of any type, the usual format is...
\begin{verbatim}
var varname = value;
\end{verbatim}
The type of varname is inferred from the type of the value. Types can be mixed
in operators and functions with some type coercion, which can be very dangerous
if used incorrectly but that's a discussion for a different part of this report.
When discussing Fay, we really only care about the composite Object data type. 
This is an object in the usual sense, with attributes and associated methods. 

Representing data in this manner allows for the concepts of Laziness and currying
to be carried over from Haskell and represented in javascript. Of course this could
be written manually in javascript without the need for Haskell or a translator, but
such programming would be tedious and error-prone. A disadvantage to this method is a 
loss of efficiency. This would not be the intended manner of programming in
javascript and the language is not necessarily optimized to deal with it. It
also produces a larger output of code than programming in a more traditional 
imperative javascript manner. This is of more signficance in web languages
such as javascript where a larger body of code will take longer to transfer
from the server to the client/browser, slowing page load times. 



* A proper syntactic and semantic subset of Haskell
* Statically typed
* Lazy
* Pure by default
* Compiles to JavaScript
* Has fundamental data types (Double, String, etc.) based upon what JS can support
* Outputs minifier-aware code for small compressed size
* Has a trivial foreign function interface to JavaScript

\subsection{iTasks? Relevant?}

\subsection{GHCJS}

\subsection{CoffeeScript}
Worth mentioning from point of view of non-functional take on the js problem

\subsection{ClojureScript}

\pagebreak

\section{A High Level Design}
\subsection{Overview}
Having looked at the problems we wish to solve and some existing solutions, we now turn our attention to the implementation of our own solution. First,
we form an image of the overall architecture of our solution. At a
very abstract, high level we can say our project will require the following:

\begin{enumerate}
	\item An input language rich enough to allow us to write programs
		  that capture our intended semantics and features.
	\item A transformation from this language into the actions we 
	 	  wish to be executed by a browser.
\end{enumerate}

\noindent The latter requirement can be decomposed into a number of sub-requirements.
We will need to split our transformation firstly into compilation and runtime phases.
A source program written in our input language will first be compiled into some runnable
representation thereof by a compilation stage. This representation of our program will
then be executed by our runtime until a value is returned. We can be somewhat vague
for the moment about the exact details of the input language and compilation phase, 
but we do know that our runtime must be capable of executing programs in a web browser.
We could say trivially that we only need to compile our programs in advance to simple
end values which could be interpreted by a browser, but this would leave us with a 
glorified static expression evaluator. Thus, our runtime must be executable from a
web browser. With this in mind, we can say that it is the job of our compiler to
take our input language and transform it into a representation which our runtime can
then execute when called from a browser. 

Seperating our compiler and runtime poses a small problem. We now have two disparate
stages which will need to communicate in some sense. We could write both our compiler
and runtime in a language which can be executed by a browser, but this would be 
unnecessary and an inefficient use of any such language. Instead, we should write our
compiler in a more general purpose language and find a means of converting our compiled
representation into one which can be understood by our runtime. Let us now re-examine
our project requirements.

\begin{enumerate}
	\item An input language rich enough to allow us to write programs
		  that capture our intended semantics and features.
	\item A compiler capable of transforming our input language into
		  a program representation executable by our runtime.
	\item A means of converting our compiled representation into one
		  which can be executed by our runtime.
	\item A runtime to execute our program in a browser. 
\end{enumerate}

\subsection{Input Language}

**Note we haven't commented on the format of core programs beyond **\\
**recognizing them at a glance. Need to go into detail on [ScDefn]**\\
**and others, although we could probably leave that until impl	  **\\

\noindent We will start by examining the first requirement. We will need a
language that is capable of conveying at the very least the generic basics of a programming language, namely the notions of primitive value and function
declarations and function applications. After this, our language will need
some means of flow control, namely conditionally executed statements and 
loops. The last of these basic required features is some manner of stuctured
data type, allowing for the creation of more complex types as composites of
primitive types. 

Our resultant compiled language will need to type-safe and support lazy
evaluation. The former will require some variety of type-checking to be 
performed on our input language at some stage during compilation, and any
type errors found to be dealt with and report. The latter is more of a 
compilation concern than one one of language choice, although a pure input
language which we can guarantee side-effect free would be of great help.

At this point, we can start to see certain parallels with our ideal input
language and currently existing programming languages. What we have described
looks a lot like the generic template for functional languages such as Miranda
or Haskell, with a very simple feature set. It could also be compared to 
classic LISP with the addition of structured data types. 

On a more interesting note is the similarity of our required language to
certain subsets of Lambda Calculus. Before examing these subsets, we'll
take a quick look at some simpler dialects. The basic
features of simple untyped lambda calculus are lambda terms, denoting well formed expressions in the lambda calculus which can consist of:

\begin{itemize}
\item Variables
\item Lambda abstractions
\item Lambda applications
\end{itemize}

\noindent where the latter two correspond with function abstractions (or
definitions) and applications in more familar terminology. The lambda term
representing a function that takes two parameters and returns their sum 
would be 

\begin{center} 
	\( \lambda x y \rightarrow x + y \)
\end{center}

\noindent The typed varieties of lambda calculus add the concepts of types and type
notations, updating the syntax with a new construct \(x:\Gamma \) indicating
a variable \(x\) of type \(\Gamma\). This would be the only significant 
difference in the simply typed dialect. 

Let us now look at a variant of the language known as System F. This dialect
adds the notion of type polymorphism or universal type quantification to 
the simply typed lambda calculus. The effect of this is to allow for the
use of variables which range over types as well as functions, in comparison
to the symply typed lambda calculus where variables only range over functions.
In practice this allows us to reason about the types of functions, which also
provides us with the ability to write functions which range over universally
quantified types. As an example, we can express the identity function as

\begin{center}
 \( \Lambda \alpha . \lambda x^\alpha . x : \forall \alpha . \alpha \rightarrow \alpha  \)
\end{center}

\noindent which we read as \emph{the type level function which takes 
the type \(\alpha\) and returns the id function \( \lambda x^\alpha . x \)
(which is of type \emph{ function that takes a paramater of type \( \alpha \) 
and forall type \(\alpha\) returns a value of type \( \alpha \)})}, where 
the id function can be read as \emph{the function that takes an x of 
type \( \alpha \) and returns same}. Those interested will find further information
on System F here LINKLINK

The reason System F is so intereesting to us is its similarity to a number of functional
language representations currently in use. Many high-level functional programming languages
can be expressed in terms of System F while still preserving their full semantics. Of
particular note however are the languages of Core and SAPL. These are intermediate languages
used as minimal representations for Haskell and Clean respectively which can be emitted by
specific compilers for each language midway through compilation. The term 'Core' is often
used to refer to any such intermediate functional language based on typed lambda calculus,
but from here on we shall refer to the GHC (Glasgow Haskell Compiler) specific intermediate
language as Core or 'External Core' and similar languages as 'Core-like'. Core is a lambda
calculus dialect (System F with added type coercions to be specific) which can be obtained
from GHC with the compiler argument -fext-core. SAPL can be similarly obtained from the 
Clean compiler. 


\noindent The semantics of Core look like this..

\begin{verbatim}
type CoreExpr = Expr Var

data Expr b 
  = Var	  Id
  | Lit   Literal
  | App   (Expr b) (Arg b)
  | Lam   b (Expr b)
  | Let   (Bind b) (Expr b)
  | Case  (Expr b) b Type [Alt b]
  | Cast  (Expr b) Coercion
  | Note  Note (Expr b)
  | Type  Type
\end{verbatim}

\noindent ...when expressed in terms of a Haskell algebraic data type (which 
coincidentally is a nice way to express most things in life). Lam in this
case stands for Lambda and the liberally added 'b's for the types of the binders
in the expression, the rest is mostly self explanatory. Note that data constructors
are not represented here although are included in the language seperate to the
Core expr type, along with a \%data denotation.

By the time a source program reaches the stage of GHC where it can be emitted
as Core, it has already undergone type checking. It has also been substantially 
minimized to the simplest representation
possible still perserving all of the original program's semantics. This makes
Core an excellent candidate as an input language, as its minimalism makes it
easy to parse and allows us to make assumptions about its type safety.

To give an idea of what a Core program looks like, we'll examine a small 
example. A factorial program is the canonical functional Hello World and will
demonstrate a few important concepts, so lets go with that. Our source program
will be 

\begin{verbatim}
module MIdent where

fac :: Int -> Int
fac 0 = 1
fac x = x * (fac x - 1)

main = fac 4
\end{verbatim}

\noindent Compiling this with ghc -fext-core produces the following output...

\begin{verbatim}
%module main:MIdent
  %rec
  {main:MIdent.fac :: ghczmprim:GHCziTypes.Int ->
                      ghczmprim:GHCziTypes.Int =
     \ (dsdl0::ghczmprim:GHCziTypes.Int) ->
      %case ghczmprim:GHCziTypes.Int dsdl0
      %of (wildX4::ghczmprim:GHCziTypes.Int)
        {ghczmprim:GHCziTypes.Izh (ds1dl1::ghczmprim:GHCziPrim.Intzh) ->
           %case ghczmprim:GHCziTypes.Int ds1dl1
           %of (ds2Xl6::ghczmprim:GHCziPrim.Intzh)
             {%_ ->
                base:GHCziNum.zt @ ghczmprim:GHCziTypes.Int 
                base:GHCziNum.zdfNumInt wildX4
                (base:GHCziNum.zm @ ghczmprim:GHCziTypes.Int
                 base:GHCziNum.zdfNumInt (main:MIdent.fac wildX4)
                (ghczmprim:GHCziTypes.Izh (1::ghczmprim:GHCziPrim.Intzh)));
                (0::ghczmprim:GHCziPrim.Intzh) ->
                 ghczmprim:GHCziTypes.Izh (1::ghczmprim:GHCziPrim.Intzh)}}};
  main:MIdent.main :: ghczmprim:GHCziTypes.Int =
    main:MIdent.fac
    (ghczmprim:GHCziTypes.Izh (4::ghczmprim:GHCziPrim.Intzh));
\end{verbatim}

\noindent Which is intended to be partially human readable, but mostly
just parseable. Lets tidy this up a bit...

\begin{verbatim}
%module main:MIdent
%rec
{fac :: Int -> Int =
\ (d0::Int) -> %case Int d0 %of (wildX4::Int)

{Types.Izh (d1::Int) -> %case Types.Int d1 %of (d6::Int)

{%_      ->
    * @ wildX4 (- @ (fac wildX4) 1::Int);

(0::Int) ->
    (1::Int)}}};

main :: Types.Int = fac (Types.Izh (4::Int));
\end{verbatim}

\noindent That looks nicer. Here, I have shortened the type names in the
primitive value type notations, removed the function return type notations
and replaced the longwinded arithmetic operator names with their traditional
symbols. We can see that the guards (Haskell pattern matching syntax, denoted
with '\textbar' symbols) have been removed and replaced with a case statement, in
keeping with Core's minimal mindset. We can also see a lambda calculus esque
structure, with our computations being represented in the form of \( \backslash
 [params] \rightarrow function body\). The influence System F can be seen in
 the type signatures prefixing the fac function. Of note also is the '@' symbol, used
 to denote function application. More on this later, for the moment it's enough
 to know that x + y can be represented as + @ x y, that is \emph{plus applied to
 x and y}.
 
SAPL is similar to core, with a few differences. SAPL uses a \emph{select} 
expression in place of Core's case expressions although this is a difference in name
only, the semantics of both being equivalent. SAPL also has a built in \emph{if}
expression which in Core would be expressed using case expressions instead. SAPL
contains no type annotations, all such information having been removed after the
Clean compilation type-checking stage.

Choosing a language such as Core or SAPL as our input language would provide
a number of benefits for this project. Firstly, it would allow us to write
programs in Haskell or Clean and make use of existing tools to translate these
into their Core-like representations.. This would mean that we can write our browser
programs in languages with pleasant high level syntax. We could leave the 
parsing of these programs to existing Haskell or Clean compilers and focus 
instead on the more interesting problems of compilation and runtime. We would
also get type-checking for free, as the type checking for both Core and SAPL 
would have already been performed by the time they are emitted by their respective
compilers. It would also reduce the learning curve for users who wish to use
this project who already have experience with writing programs in languages
with syntax similar to Haskell or Clean.
 
Having seen the various features of System F languages such as SAPL and Core, I
was quite content to choose such a language as the input for my own project. 
From our research into the iTasks system, we have seen that such a language 
can provide a good starting point for a project such as this. Between SAPL and
Core I was more inclined towards using the latter. I have far more experience
using Haskell than Clean. Also, though we do not need them currently, it is
worth noting that having Core's type annotations may prove useful for future
improvements to our project. Such information would be of great help when 
diagnosing runtime errors, for example. It was decided that Core would be the
input language.

\subsection{Compilation Strategy}
Now that we have decided on our input language, we need to plan our compilation
strategy. Thankfully for us, there is a wealth of literature and previous work
on writing a compiler for System F resemblant languages and a number of approaches
already exist from which we can take inspiration. An overview of general 
functional language compilation is provided in the background chapter of this
report so that this section and following sections discussing compilation can 
focus on the aspects of immediate interest to us.

*ADD GRAPH REDUCTION TO BACKGROUND*

First, we need some way to compile programs written in our input language into
some form from which we can derive an initial state. The traditional way to
do this is to view our program as a graph as described above and convert this
graph into a programmatic representation. There are many existing functional
compilation language strategies which achieve this, so it was decided to 
examine those before deciding on an implementation. In particular, there were
two strategies which I investigated in-depth

\begin{enumerate}
	\item The template instantation machine
	\item The G-Machine
\end{enumerate}

\subsubsection{The template instantiation machine} 
The template instantiation machine is a simplistic approach to
functional graph reduction and compilation but conveys many of 
the ideas used in more complicated approaches. When the project
started moving towards implemenation of a solution, this is the
approach I started with. It proved very useful in understanding
some of the key ideas in writing a functional compiler and gave
me an idea of the scope of such an implementation. 

The machine is built upon a state consisting of
\begin{itemize}
	\item A stack
	\item A dump
	\item A heap
	\item A globals array
\end{itemize}

\noindent The stack is a stack of addresses which correspond
to nodes in the heap. These nodes form the spine of the 
expression being evaluated. The dump is used to record the
state of the expression being evaluated. When we **EXPLAIN**.
The heap is a list of nodes and their associated heap addresses.
The nodes contained in the heap correspond to the examples
given in the overview in the background section.
The globals array associates the names of our declared
supercombinators with their addresses in the heap, allowing
supercombinators to be looked up by name. 

The operation of the template instiation machine is described
by a number of state transitions which are called if the state
of the machine represents that of the transition. When no
transition rule matches, we assume execution to be complete. 
At most one state transition rule will apply at any point;
more than one would imply non-determinism which is not
permissible. In general, the state transition rules are
concerned with the type of node pointed to by the top of the
stack and will react accordingly. For example, in the case
where an application node is found, the appropriate rule
is triggered which will pop the application and unwind its
two arguments onto the stack. The rule for dealing with 
supercombinators is also of interest to us. When a 
supercombinator node pointer is found on top of the stack
we instantiate it by binding the argument names of its
body to the argument addresses found on top of the stack.
We then discard the arguments from the stack, reduce the
resulting redex and either push the root of the result 
to the stack or overwrite the root of the redex with the
result, depending on whether the implementation in question
is performing lazy updates. 

The implemenation of the template instantiation machine is
split into compilation and evaluation stages. The compiler
in the first stage takes our input program, along with
any built-in prelude definitions, and builds an initial
state of the form described above. The evaluator in the 
second stage takes this initial state and runs our machine
on it one step at a time, applying the necessary state
transition rules, until we reach a final state. The stepping
function takes as input a state and returns a resultant
state. The machine is considered to have finished when 
the stack consists of a single pointer to an item which
can be evaluated no further, for example and integer or
a fully saturated data constructor. 

This is about the extent to which I implemented my initial
template instantiation machine. There were further 
improvements which could be made, such as adding primitive
operations, let expressions etc. etc. My implementation was
capable of taking a program representing an identity 
function, compiling it and evaluating it. This was 
sufficient for my purposes of understanding some of the
basic concepts of functional compiler implementation. At
this point, I was looking to move towards a different
implementation which would better suit some of my 
requirements.

**PROVIDE EXAMPLE COMPILATION**

The template instantiation machine suffers from one notable
problem which made it particularly unsuitable for our
purposes. The general operation of the template instantiation
machine constructs an instance of a supercombinator body.
Each time we attempt to instantiate a supercombinator we
recurseivly traverse the template. This action is executed
in the evaluation stage. We know that our end goal is the
ability to execute programs in a browser and as such it would
be of great benefit to us if we could minimize the amount of
work needed at run time. Thankfully for us, there exist
implemenations which are capable of doing this!

\subsubsection{The G-machine}
The G-machine differs from the template instantiation machine
in a few ways but there is one significant difference in 
their principles of operation. The G-machine translates each
supercombinator body to a sequence of instructions which will
construct an instance of the supercombinator body when run.
In comparison to the template instantiation machine, this
allows us to execute the actions of a supercombinator without
the need to instantiate it at run time, this having been
achieved in advance at compile time. 

The G-machine decouples the compiler from evaluator to a
greater extent than the template instantion machine. The
G-machine's compiler not only produces an initial state from
our input program but also a list of instructions which when
combined with the initial state can be evaluated to a final
state. The set of instructions drawn from is designed to be
minimal. The instructions and state produced
by the compiler can be said to represent an \emph{abstract
machine} which can then be implemented on various different
architectures and platforms. This makes it easier to write
runtime evaluators for G-machine compiled programs, as the
initial state can be expressed in an intermediate form 
between that of a reducible graph state representation 
and an easily executed program state.

The general form of the G-machine is similar to that of
the template instantiation machine. We take our input language
and compile this into an initial state. This state is identical
to the one listed previously, with the addition of a sequence
of instructions to be called in order to evaluate our state.
There still exists a heap and stack, the former still containing
nodes of the same types as those previously encountered.
This state is passed to the runtime which will execute until
we reach a final state where we have no further instructions
to evaluate and a single pointer to a node in the heap on top
of the stack. Our runtime must now possess the means to 
interpret these instructions and execute them upon the current
state, producing a new state. This is in contrast to the 
evaluation stage of the template instantiation machine which
had a list of state transition rules which would fire when
the state matched that of one of the rules. Here, only the
initial state was necessary for evaluation.

The following are instructions one would find in a
basic G-machine implementation. 

\begin{itemize}
	\item Unwind: unwinds the spine in much the usual
		  manner.
	\item PushGlobal: finds a supercombinator by name 
		  from the globals array and places its heap
		  address on top of the stack.
	\item PushInt: Places an integer node in the heap
		  and its address on top of the stack.
	\item Mkap: Forms an application node from the
		  top two node pointers on the stack.
	\item Slide: drops N pointers from the stack behind
		  the current top pointer. Used to remove
		  arguments pointers after evaluating a 
		  supercombinator.
	\item Update: updates the root of a redex with its
		  reduced value. Used to implement laziness.
\end{itemize}

\noindent More in-depth information on these and other 
instructions can be found in later implementation chapters. 

The G-machine compiler consists of a number of schemes which
determine what sequence of instructions to output when we
encounter certain expressions in our input language. These
together with an initial heap built from the supercombinators
of our input program along with any prelude definitions are
then passed to the runtime.

\subsection{Runtime}


\section{Implementation}







\begin{comment}

\section{Description of implementation and choices}

\subsection{A subset language as a Haskell DSL}
This will 

\subsection{Syntax}
One of the earlier considerations for this project, by virtue of the natural order
of implementing features in \lang, was the syntax of the developer facing input 
language (the "language" itself really). I knew from the outset that the syntax
used in javascript was not something I wished to reinvent, seeing it more as a
problem than an inspiration. 

The syntax in javascript feels like a relic of a different time. It is a common
problem brought up by programmers new to the language, and an accepted
hindrance for anyone more experience with javascript. 

Keywords such as 'new', 'var', and 'function' serve as examples of ideas on syntax
design which have become far less common since javascript was created. 'var', for
instance, adds verbosity to the language while providing little benefit. Seemingly
inspired by Scheme's 'define' concept, it adds additional cruft to declaration 
statements which are otherwise self-explanatory in languages with more minimal 
syntaxes such as Python. A declaration can be inferred sufficiently from a simple
"x = y" statement and its context. 'function' as a keyword also seems to have been
inspired by Scheme, in this case its liberal use of the 'lambda' keyword to represent
anonymous functions. Languages such as Haskell dont overcomplicate their function
defintions, content with statements as simple and declarative as 'f x y = x + y',
as well as internally scoped (fix that terminology when less tired) functions using
the where keyword etc.
Type signatures are optional but do serve to make the code in question more readable and
solve some programmer headaches (from a syntactical point of view, they obviously have other 
significant semantic effects). 

Both Fay and CoffeeScript went the minimal, Pythonic route with syntax design. Statements
such as...
\begin{itemize}
\item Variable declarations : "\(number = 42\)"
\item Function declarations : "\(square = (x) \rightarrow x * x\)"
\item List comprehensions : "\(cubes = (math.cube num \emph{for} num \emph{in} list)\)"
\end{itemize}

...in coffeescript contrast sharply with the parentheses, "function" and semicolon 
ridden expanse of comparable javascript. The Fay approach is similar in its brevity.

My approach to syntax design would be influenced by these ideas. They are reflected 
in the simple declarative statements included in my language, and were I to continue
this project beyond its current scope the same concepts would continue to be applied.


\subsection{Core/STG}
link to SPJ etc. papers. Talk about SAPL and the iTasks project.

Of note: "Core: An External Representation" for just about all of this.

\subsection{Representing laziness?}
Fay, thunks-as-JS-functions

\subsection{Flow control}
case statement continuations, link to paper

\subsection{Higher order functions}

\end{comment}


\end{document}
